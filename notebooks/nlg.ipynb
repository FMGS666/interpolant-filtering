{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils.config import configuration\n",
    "from int_filt.utils.utils import ensure_reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}\n",
    "\n",
    "## defining settings\n",
    "num_dims = 1\n",
    "b_net_amortized = True\n",
    "sigma_x = 1e-2\n",
    "sigma_y = 1e-2\n",
    "\n",
    "## plotting\n",
    "num_observations_to_plot = 1000\n",
    "num_iters_to_plot = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpolant_method': 'pffp_v0', 'num_samples': 100, 'backbone': 'mlp', 'b_net_hidden_dims': [16], 'b_net_activation': ReLU(), 'b_net_activate_final': False, 'b_net_amortized': True, 'experiment': 'nlg', 'sigma_x': 0.01, 'sigma_y': 0.01, 'beta': 1.0, 'num_dims': 1, 'num_sims': 10000, 'num_iters': 50000, 'non_linearity': 'exp', 'step_size': 0.01, 'log_dir': './out/2024-06-12/run_2024-06-12_02-07-30', 'dump_dir': 'exp/2024-06-12/run_2024-06-12_02-07-30', 'b_net_num_grad_steps': 250, 'b_net_optimizer': 'adam-w', 'b_net_scheduler': 'none', 'b_net_lr': 0.001, 'random_seed': 128, 'device': device(type='cuda'), 'mc_config': {'num_samples': 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49999/49999 [00:07<00:00, 6335.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"exp\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_exp = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "#fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad step: 0, Velocity Field MSE Loss: 11.426286697387695\n",
      "Grad step: 5, Velocity Field MSE Loss: 11.156567573547363\n",
      "Grad step: 10, Velocity Field MSE Loss: 11.037060737609863\n",
      "Grad step: 15, Velocity Field MSE Loss: 10.956230163574219\n",
      "Grad step: 20, Velocity Field MSE Loss: 10.94946002960205\n",
      "Grad step: 25, Velocity Field MSE Loss: 10.932168960571289\n",
      "Grad step: 30, Velocity Field MSE Loss: 10.839601516723633\n",
      "Grad step: 35, Velocity Field MSE Loss: 10.912629127502441\n",
      "Grad step: 40, Velocity Field MSE Loss: 10.912618637084961\n",
      "Grad step: 45, Velocity Field MSE Loss: 10.980607986450195\n",
      "Grad step: 50, Velocity Field MSE Loss: 10.893630981445312\n",
      "Grad step: 55, Velocity Field MSE Loss: 10.855619430541992\n",
      "Grad step: 60, Velocity Field MSE Loss: 10.869399070739746\n",
      "Grad step: 65, Velocity Field MSE Loss: 10.857954025268555\n",
      "Grad step: 70, Velocity Field MSE Loss: 10.831671714782715\n",
      "Grad step: 75, Velocity Field MSE Loss: 10.83996295928955\n",
      "Grad step: 80, Velocity Field MSE Loss: 10.82258129119873\n",
      "Grad step: 85, Velocity Field MSE Loss: 10.744264602661133\n",
      "Grad step: 90, Velocity Field MSE Loss: 10.727758407592773\n",
      "Grad step: 95, Velocity Field MSE Loss: 10.593942642211914\n",
      "Grad step: 100, Velocity Field MSE Loss: 10.578558921813965\n",
      "Grad step: 105, Velocity Field MSE Loss: 10.513168334960938\n",
      "Grad step: 110, Velocity Field MSE Loss: 10.563594818115234\n",
      "Grad step: 115, Velocity Field MSE Loss: 10.388599395751953\n",
      "Grad step: 120, Velocity Field MSE Loss: 10.450493812561035\n",
      "Grad step: 125, Velocity Field MSE Loss: 10.343878746032715\n",
      "Grad step: 130, Velocity Field MSE Loss: 10.236297607421875\n",
      "Grad step: 135, Velocity Field MSE Loss: 10.314711570739746\n",
      "Grad step: 140, Velocity Field MSE Loss: 10.24679183959961\n",
      "Grad step: 145, Velocity Field MSE Loss: 10.164020538330078\n",
      "Grad step: 150, Velocity Field MSE Loss: 10.154216766357422\n",
      "Grad step: 155, Velocity Field MSE Loss: 10.13385009765625\n",
      "Grad step: 160, Velocity Field MSE Loss: 10.13333797454834\n",
      "Grad step: 165, Velocity Field MSE Loss: 10.043417930603027\n",
      "Grad step: 170, Velocity Field MSE Loss: 10.10423469543457\n",
      "Grad step: 175, Velocity Field MSE Loss: 9.993882179260254\n",
      "Grad step: 180, Velocity Field MSE Loss: 9.981082916259766\n",
      "Grad step: 185, Velocity Field MSE Loss: 9.99179744720459\n",
      "Grad step: 190, Velocity Field MSE Loss: 9.892513275146484\n",
      "Grad step: 195, Velocity Field MSE Loss: 9.883670806884766\n",
      "Grad step: 200, Velocity Field MSE Loss: 9.867733001708984\n",
      "Grad step: 205, Velocity Field MSE Loss: 9.843913078308105\n",
      "Grad step: 210, Velocity Field MSE Loss: 9.756844520568848\n",
      "Grad step: 215, Velocity Field MSE Loss: 9.763998031616211\n",
      "Grad step: 220, Velocity Field MSE Loss: 9.823588371276855\n",
      "Grad step: 225, Velocity Field MSE Loss: 9.67111587524414\n",
      "Grad step: 230, Velocity Field MSE Loss: 9.662349700927734\n",
      "Grad step: 235, Velocity Field MSE Loss: 9.60534381866455\n",
      "Grad step: 240, Velocity Field MSE Loss: 9.54131031036377\n",
      "Grad step: 245, Velocity Field MSE Loss: 9.487692832946777\n"
     ]
    }
   ],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_exp.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_exp.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_exp.b_net.state_dict(), os.path.join(dump_dir, \"b_net_exp.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
