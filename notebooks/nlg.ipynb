{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils.config import configuration\n",
    "from int_filt.utils.utils import ensure_reproducibility, move_batch_to_device, create_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None,\n",
    "    \"cosine_annealing\": torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}\n",
    "\n",
    "## defining simulation settings\n",
    "non_linearity = \"exp\"\n",
    "num_dims = 1\n",
    "num_sims = 1_000\n",
    "#num_iters = 1_000_000\n",
    "num_iters = 500_000\n",
    "sigma_x = 1e-2\n",
    "sigma_y = 1e-2\n",
    "beta = 1e-0\n",
    "\n",
    "## defining plotting settings\n",
    "num_observations_to_plot = 1_000\n",
    "num_iters_to_plot = 200\n",
    "\n",
    "## defining model settings\n",
    "b_net_amortized = True\n",
    "b_net_lr = 1e-3\n",
    "b_net_num_grad_steps = 300\n",
    "\n",
    "## defining sampling settings\n",
    "num_samples = 300\n",
    "num_time_steps = 300\n",
    "\n",
    "## defining mc estimation settings\n",
    "num_mc_samples = 500\n",
    "\n",
    "## defining optimizer and scheduler settings\n",
    "scheduler = \"none\"\n",
    "optimizer = \"adam-w\"\n",
    "\n",
    "## defining preprocessing settings\n",
    "preprocessing = \"sim\"\n",
    "#preprocessing = \"history\"\n",
    "#preprocessing = \"batch\"\n",
    "#preprocessing = \"fixed-std\"\n",
    "#preprocessing = \"none\"\n",
    "#preprocessing = \"fixed-std-zero-mean\"\n",
    "\n",
    "## defining hiden dimensions\n",
    "hidden_widths = [500]\n",
    "#hidden_widths = [500]*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_config = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": non_linearity,\n",
    "    \"num_dims\": num_dims,\n",
    "    \"num_sims\": num_sims,\n",
    "    \"num_iters\": num_iters,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y,\n",
    "    \"beta\": beta,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"b_net_lr\": b_net_lr,\n",
    "    \"b_net_num_grad_steps\": b_net_num_grad_steps,\n",
    "    \"b_net_scheduler\": scheduler,\n",
    "    \"b_net_optimizer\": optimizer,\n",
    "    \"b_net_hidden_dims\": hidden_widths,\n",
    "    \"num_samples\": num_mc_samples,\n",
    "    \"preprocessing\": preprocessing,\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_config.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_exp = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "#latent_states_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "#observations_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "#observation_indices = np.arange(num_iters_to_plot)\n",
    "#latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "#fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)\n",
    "#plt.plot(experiment_nlg_exp.ssm.sim[\"latent_states\"][:, 0, 0])\n",
    "#plt.plot(experiment_nlg_exp.ssm.sim[\"observations\"][:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = experiment_nlg_exp.get_batch()\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment_nlg_exp.preprocessing(batch)\n",
    "print(\"\\nAFTER STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer and scheduler\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_exp.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "b_net_scheduler = SCHEDULERS[b_net_scheduler]\n",
    "if b_net_scheduler is not None:\n",
    "    b_net_scheduler = b_net_scheduler(b_net_optimizer, b_net_num_grad_step)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_exp.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_exp.b_net.state_dict(), os.path.join(dump_dir, \"b_net_exp.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sample_config = {\n",
    "    \"num_time_steps\": num_time_steps,\n",
    "    \"num_samples\": num_samples\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment_nlg_exp.get_batch()\n",
    "batch = move_batch_to_device(batch, experiment_nlg_exp.device)\n",
    "\n",
    "## sampling from model\n",
    "samples = experiment_nlg_exp.sample(batch, sample_config = sample_config)\n",
    "\n",
    "## sampling from gt state transition\n",
    "x = batch[\"x0\"]\n",
    "samples_gt = torch.zeros_like(samples)\n",
    "for sample_id in tqdm(range(num_samples)):\n",
    "    x1 = experiment_nlg_exp.ssm.state_transition(x)\n",
    "    samples_gt[sample_id] = x1\n",
    "\n",
    "## plotting example histogram\n",
    "gt_dist = samples_gt[:, 0, 0]\n",
    "predicted_dist = samples[:, 0, 0]\n",
    "print(f\"{gt_dist.shape=}, {predicted_dist.shape}\")\n",
    "print(f\"{gt_dist.mean()=}, {predicted_dist.mean()=}\")\n",
    "print(f\"{gt_dist.std()=}, {predicted_dist.std()=}\")\n",
    "sns.kdeplot(gt_dist, label = \"ground truth\")\n",
    "sns.kdeplot(predicted_dist, label = \"prediction\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
