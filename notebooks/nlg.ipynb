{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils.config import configuration\n",
    "from int_filt.utils.utils import ensure_reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}\n",
    "\n",
    "## defining settings\n",
    "num_dims = 1\n",
    "#b_net_amortized = True\n",
    "b_net_amortized = False\n",
    "sigma_x = 1e-2\n",
    "sigma_y = 1e-2\n",
    "\n",
    "## plotting\n",
    "num_observations_to_plot = 1000\n",
    "num_iters_to_plot = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpolant_method': 'pffp_v0', 'num_samples': 100, 'backbone': 'mlp', 'b_net_hidden_dims': [16], 'b_net_activation': ReLU(), 'b_net_activate_final': False, 'b_net_amortized': True, 'experiment': 'nlg', 'sigma_x': 0.01, 'sigma_y': 0.01, 'beta': 1.0, 'num_dims': 1, 'num_sims': 10000, 'num_iters': 50000, 'non_linearity': 'exp', 'step_size': 0.01, 'log_dir': './out/2024-06-12/run_2024-06-12_12-40-34', 'dump_dir': 'exp/2024-06-12/run_2024-06-12_12-40-34', 'b_net_num_grad_steps': 2000, 'b_net_optimizer': 'adam-w', 'b_net_scheduler': 'none', 'b_net_lr': 0.001, 'random_seed': 128, 'device': device(type='cuda'), 'mc_config': {'num_samples': 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49999/49999 [00:13<00:00, 3805.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"exp\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"b_net_lr\": 1e-3,\n",
    "    \"b_net_num_grad_steps\": 2000\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_exp = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "#fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE STANDARDIZATION\n",
      "\n",
      "x0 -> mean:  tensor(5.4229) , std:  tensor(1.4124)\n",
      "x1 -> mean:  tensor(5.4229) , std:  tensor(1.4123)\n",
      "xc -> mean:  tensor(5.4229) , std:  tensor(1.4124)\n",
      "y -> mean:  tensor(5.4229) , std:  tensor(1.4124)\n",
      "\n",
      "AFTER STANDARDIZATION\n",
      "\n",
      "x0 -> mean:  tensor(-2.6334e-05) , std:  tensor(1.2679)\n",
      "x1 -> mean:  tensor(-3.2826e-05) , std:  tensor(1.2677)\n",
      "xc -> mean:  tensor(-2.6334e-05) , std:  tensor(1.2679)\n",
      "y -> mean:  tensor(-2.8455e-05) , std:  tensor(1.2679)\n"
     ]
    }
   ],
   "source": [
    "batch = experiment_nlg_exp.get_batch()\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std())\n",
    "batch = experiment_nlg_exp.standardize(batch)\n",
    "print(\"\\nAFTER STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grad Step 2000/2000, MSELoss: 0.7678613662719727: 100%|██████████| 2000/2000 [06:19<00:00,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_exp.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_exp.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_exp.b_net.state_dict(), os.path.join(dump_dir, \"b_net_exp.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
