{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils.config import configuration\n",
    "from int_filt.utils.utils import ensure_reproducibility, move_batch_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None,\n",
    "    \"cosine_annealing\": torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}\n",
    "\n",
    "## defining simulation settings\n",
    "non_linearity = \"exp\"\n",
    "num_dims = 1\n",
    "num_sims = 1_000\n",
    "num_iters = 1_000_000\n",
    "sigma_x = 1e-2\n",
    "sigma_y = 1e-2\n",
    "beta = 1e-0\n",
    "\n",
    "## defining plotting settings\n",
    "num_observations_to_plot = 1_000\n",
    "num_iters_to_plot = 200\n",
    "\n",
    "## defining model settings\n",
    "b_net_amortized = True\n",
    "b_net_lr = 1e-3\n",
    "b_net_num_grad_steps = 300\n",
    "\n",
    "## defining sampling settings\n",
    "num_samples = 100\n",
    "num_time_steps = 300\n",
    "\n",
    "## defining mc estimation settings\n",
    "num_mc_samples = 300\n",
    "\n",
    "## defining optimizer and scheduler settings\n",
    "scheduler = \"none\"\n",
    "optimizer = \"adam-w\"\n",
    "\n",
    "## defining preprocessing settings\n",
    "preprocessing = \"sim\"\n",
    "\n",
    "## defining hiden dimensions\n",
    "hidden_widths = [500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interpolant_method': 'pffp_v0', 'num_samples': 300, 'backbone': 'mlp', 'b_net_hidden_dims': [500], 'b_net_activation': ReLU(), 'b_net_activate_final': False, 'b_net_amortized': True, 'experiment': 'nlg', 'sigma_x': 0.01, 'sigma_y': 0.01, 'beta': 1.0, 'num_dims': 1, 'num_sims': 10000, 'num_iters': 1000000, 'non_linearity': 'exp', 'step_size': 0.01, 'log_dir': './out/2024-06-14/run_2024-06-14_11-25-21', 'dump_dir': 'exp/2024-06-14/run_2024-06-14_11-25-21', 'b_net_num_grad_steps': 300, 'b_net_optimizer': 'adam-w', 'b_net_scheduler': 'none', 'b_net_lr': 0.001, 'random_seed': 128, 'device': device(type='cuda'), 'preprocessing': 'sim', 'mc_config': {'num_samples': 300}}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m## creating experiment\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m experiment_nlg_exp \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m## joyplot\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m## retrieving data\u001b[39;00m\n\u001b[1;32m     59\u001b[0m latent_states_nlg_exp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(experiment_nlg_exp\u001b[38;5;241m.\u001b[39mssm\u001b[38;5;241m.\u001b[39msim[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_states\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/Projects/interpolant-filtering/notebooks/../int_filt/experiments/utils.py:145\u001b[0m, in \u001b[0;36mcreate_experiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m## initializing gaussian model \u001b[39;00m\n\u001b[1;32m    136\u001b[0m ssm_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_x\u001b[39m\u001b[38;5;124m\"\u001b[39m: sigma_x,\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_y\u001b[39m\u001b[38;5;124m\"\u001b[39m: sigma_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: step_size\n\u001b[1;32m    144\u001b[0m }\n\u001b[0;32m--> 145\u001b[0m ssm \u001b[38;5;241m=\u001b[39m \u001b[43mNON_LINEARITIES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_linearity\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m## initializing interpolant\u001b[39;00m\n\u001b[1;32m    147\u001b[0m interpolant_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: interpolant_method}\n",
      "File \u001b[0;32m~/Projects/interpolant-filtering/notebooks/../int_filt/src/ssm/non_linear_gaussian.py:200\u001b[0m, in \u001b[0;36mSimNLGExp.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28msuper\u001b[39m(SimNLGExp, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m## running the simulations\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/interpolant-filtering/notebooks/../int_filt/src/ssm/non_linear_gaussian.py:64\u001b[0m, in \u001b[0;36mSimNonLinearGaussian.simulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mRuns the simulation\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m## allocating memory\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m latent_states_store \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_sims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m observation_store \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dims))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m## sampling first states and observations\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "## defining configurations\n",
    "experiment_config = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": non_linearity,\n",
    "    \"num_dims\": num_dims,\n",
    "    \"num_sims\": num_sims,\n",
    "    \"num_iters\": num_iters,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y,\n",
    "    \"beta\": beta,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"b_net_lr\": b_net_lr,\n",
    "    \"b_net_num_grad_steps\": b_net_num_grad_steps,\n",
    "    \"b_net_scheduler\": scheduler,\n",
    "    \"b_net_optimizer\": optimizer,\n",
    "    \"b_net_hidden_dims\": hidden_widths,\n",
    "    \"num_samples\": num_mc_samples,\n",
    "    \"preprocessing\": preprocessing\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_config.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_exp = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "#fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)\n",
    "#plt.plot(experiment_nlg_exp.ssm.sim[\"latent_states\"][:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = experiment_nlg_exp.get_batch()\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment_nlg_exp.preprocessing(batch)\n",
    "print(\"\\nAFTER STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer and scheduler\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_exp.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "b_net_scheduler = SCHEDULERS[b_net_scheduler]\n",
    "if b_net_scheduler is not None:\n",
    "    b_net_scheduler = b_net_scheduler(b_net_optimizer, b_net_num_grad_step)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_exp.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_exp.b_net.state_dict(), os.path.join(dump_dir, \"b_net_exp.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sample_config = {\n",
    "    \"num_time_steps\": num_time_steps,\n",
    "    \"num_samples\": num_samples\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment_nlg_exp.get_batch()\n",
    "batch = move_batch_to_device(batch, experiment_nlg_exp.device)\n",
    "\n",
    "## sampling from model\n",
    "samples = experiment_nlg_exp.sample(batch, sample_config = sample_config)\n",
    "\n",
    "## sampling from gt state transition\n",
    "x = batch[\"x0\"]\n",
    "samples_gt = torch.zeros_like(samples)\n",
    "for sample_id in tqdm(range(num_samples)):\n",
    "    x1 = experiment_nlg_exp.ssm.state_transition(x)\n",
    "    samples_gt[sample_id] = x1\n",
    "\n",
    "## plotting example histogram\n",
    "gt_dist = samples_gt[:, 0, 0]\n",
    "predicted_dist = samples[:, 0, 0]\n",
    "print(f\"{gt_dist.shape=}, {predicted_dist.shape}\")\n",
    "print(f\"{gt_dist.mean()=}, {predicted_dist.mean()=}\")\n",
    "print(f\"{gt_dist.std()=}, {predicted_dist.std()=}\")\n",
    "sns.kdeplot(gt_dist, label = \"ground truth\")\n",
    "sns.kdeplot(predicted_dist, label = \"prediction\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
