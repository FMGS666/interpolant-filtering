{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import gc \n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "\n",
    "from int_filt.utils import (\n",
    "    configuration, \n",
    "    ensure_reproducibility, \n",
    "    dump_config,\n",
    "    dump_tensors, \n",
    "    move_batch_to_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining macros\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"sgd\": torch.optim.SGD,\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None,\n",
    "    \"cosine-annealing\": torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "DEBUG = True\n",
    "experiment_config = {\n",
    "    ## simulation config\n",
    "    \"experiment\": \"nlg-controlled\",\n",
    "    \"controlled\": True,\n",
    "    \"non_linearity\": \"sin\",\n",
    "    \"num_dims\": 1,\n",
    "    \"num_sims\": 1_000,\n",
    "    \"num_iters\": 10_000 if DEBUG else 1_000_000,\n",
    "    \"num_burn_in_steps\": 0,\n",
    "    \"step_size\": 1e-3,\n",
    "    \"sigma_x\": 1e-2,\n",
    "    \"sigma_y\": 1e-0,\n",
    "    \"beta\": 1e-0,\n",
    "    ## interpolant config\n",
    "    \"epsilon\": 1.8e-2,\n",
    "    \"interpolant_method\": \"pffp_v0\",\n",
    "    ## model config\n",
    "    \"b_net_amortized\": False,\n",
    "    \"b_net_activation\": \"relu\",\n",
    "    \"b_net_hidden_dims\": [1028],\n",
    "    \"b_net_activate_final\": False,\n",
    "    \"c_net_activation\": \"relu\",\n",
    "    \"c_net_hidden_dims\": [1028],\n",
    "    \"c_net_activate_final\": False,\n",
    "    ## training config\n",
    "    \"num_grad_steps\": 3 if DEBUG else 100,\n",
    "    \"b_net_lr\": 1e-3,\n",
    "    \"b_net_scheduler\": \"none\",\n",
    "    \"b_net_optimizer\": \"adam-w\",\n",
    "    \"c_net_lr\": 1e-3,\n",
    "    \"c_net_scheduler\": \"none\",\n",
    "    \"c_net_optimizer\": \"adam-w\",\n",
    "    ## mc estimation config\n",
    "    \"num_mc_samples\": 50 if DEBUG else 750,\n",
    "    ## preprocessing config\n",
    "    \"preprocessing\": \"sim\",\n",
    "    ## sampling config\n",
    "    \"num_samples\": 10 if DEBUG else 450,\n",
    "    \"num_time_steps\": 10 if DEBUG else 1_000,\n",
    "    \"num_ar_steps\": 10 if DEBUG else 100,\n",
    "    \"full_out\": False,\n",
    "    \"initial_time_step\": 0,\n",
    "    \"ar_sample_train\": False,\n",
    "    ## logging config\n",
    "    \"log_results\": False,\n",
    "    ## memory config\n",
    "    \"clear_memory\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parsing arguments\n",
    "config = configuration(args=[])\n",
    "## creating experiment\n",
    "config = vars(config)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_config.items():\n",
    "    config[k] = v\n",
    "## retrieving activation and device\n",
    "config[\"b_net_activation\"] = ACTIVATIONS[config[\"b_net_activation\"]]\n",
    "config[\"c_net_activation\"] = ACTIVATIONS[config[\"c_net_activation\"]]\n",
    "config[\"device\"] = DEVICES[config[\"device\"]]\n",
    "## adding mc configuration\n",
    "config[\"mc_config\"] = {\"num_mc_samples\": config[\"num_mc_samples\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running simulation\n",
    "## setting reproducibility\n",
    "ensure_reproducibility(config[\"random_seed\"])\n",
    "## creating experiment\n",
    "experiment = create_experiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardization\n",
    "batch = experiment.get_batch()\n",
    "print(f\"STANDARDIZATION: {experiment.preprocessing.params}\")\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing(batch)\n",
    "print(\"\\nAFTER PREPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing.unstandardize(batch)\n",
    "print(\"\\nAFTER POSTPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "## initializing optimizer and scheduler\n",
    "b_net_optimizer = OPTIMIZERS[config[\"b_net_optimizer\"]](experiment.b_net.backbone.parameters(), lr = config[\"b_net_lr\"])\n",
    "b_net_scheduler = SCHEDULERS[config[\"b_net_scheduler\"]]\n",
    "if b_net_scheduler is not None:\n",
    "    b_net_scheduler = b_net_scheduler(b_net_optimizer, config[\"b_net_num_grad_steps\"])\n",
    "## initializing optimizer and scheduler for optional control term\n",
    "c_net_optimizer = None\n",
    "c_net_scheduler = None\n",
    "if config[\"controlled\"]:\n",
    "    c_net_optimizer = OPTIMIZERS[config[\"c_net_optimizer\"]](experiment.c_net.model.backbone.parameters(), lr = config[\"c_net_lr\"])\n",
    "    c_net_scheduler = SCHEDULERS[config[\"c_net_scheduler\"]]\n",
    "    if c_net_scheduler is not None:\n",
    "        c_net_scheduler = c_net_scheduler(c_net_optimizer, config[\"num_grad_steps\"])\n",
    "## constructing optimization config dictionary\n",
    "optim_config = {\n",
    "    \"b_net_optimizer\": b_net_optimizer,\n",
    "    \"b_net_scheduler\": b_net_scheduler,\n",
    "    \"c_net_optimizer\": c_net_optimizer,\n",
    "    \"c_net_scheduler\": c_net_scheduler,\n",
    "    \"num_grad_steps\": config[\"num_grad_steps\"],\n",
    "}\n",
    "## training\n",
    "if config[\"controlled\"]:\n",
    "    train_dict = experiment.train_controlled(optim_config)\n",
    "else:\n",
    "    train_dict = experiment.train_drift(optim_config)\n",
    "## optional logging\n",
    "if config[\"log_results\"]:\n",
    "    ## saving the model\n",
    "    torch.save(experiment.b_net.state_dict(), os.path.join(config[\"dump_dir\"], \"b_net.pt\"))\n",
    "    if config[\"controlled\"]:\n",
    "        torch.save(experiment.c_net.state_dict(), os.path.join(config[\"dump_dir\"], \"c_net.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting training history\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(train_dict[\"b_loss_history\"], label = \"drift loss\")\n",
    "axes.plot(train_dict[\"c_loss_history\"], label = \"control loss\")\n",
    "axes.set_title(\"Loss History\")\n",
    "axes.set_xlabel(\"Gradient Step\")\n",
    "axes.set_ylabel(\"Loss Value\")\n",
    "legend = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sde_config = {\n",
    "    \"num_time_steps\": config[\"num_time_steps\"],\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment.get_batch(train = False)\n",
    "batch = move_batch_to_device(batch, experiment.device)\n",
    "\n",
    "## simulating sde on sample batch\n",
    "sde_dict = experiment.simulate_sde(batch, config = sde_config)\n",
    "## displaying results shape\n",
    "for key, tensor in sde_dict.items():\n",
    "    print(key, f\": {tensor.shape}\")\n",
    "    ## plotting aggregated drift history over optimization\n",
    "if config[\"full_out\"]:\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_trajectory = torch.mean(sde_dict[\"trajectory\"], dim = 1)\n",
    "    std_trajectory = torch.std(sde_dict[\"trajectory\"], dim = 1)\n",
    "    ## defining figure and axis\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.set_title(\"Simulated trajectories\")\n",
    "    axes.plot(sde_dict[\"trajectory\"][:, :, 0])\n",
    "    axes.plot(mean_trajectory, label = \"mean\")\n",
    "    axes.plot(mean_trajectory + std_trajectory, color = \"red\", label = \"+- std\")\n",
    "    axes.plot(mean_trajectory - std_trajectory, color = \"red\")\n",
    "    axes.set_xlabel(\"Time Step\")\n",
    "    axes.set_ylabel(\"Simulated Trajectory\")\n",
    "    legend = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sde_config = {\n",
    "    \"num_time_steps\": config[\"num_time_steps\"],\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment.get_batch(train = False)\n",
    "batch = move_batch_to_device(batch, experiment.device)\n",
    "\n",
    "## simulating sde on sample batch\n",
    "sde_dict = experiment.simulate_controlled_sde(batch, config = sde_config)\n",
    "## displaying results shape\n",
    "for key, tensor in sde_dict.items():\n",
    "    print(key, f\": {tensor.shape}\")\n",
    "    ## plotting aggregated drift history over optimization\n",
    "if config[\"full_out\"]:\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_trajectory = torch.mean(sde_dict[\"trajectory\"], dim = 1)\n",
    "    std_trajectory = torch.std(sde_dict[\"trajectory\"], dim = 1)\n",
    "    ## defining figure and axis\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    axes[0].set_title(\"Simulated trajectories\")\n",
    "    axes[0].plot(sde_dict[\"trajectory\"][:, :, 0])\n",
    "    axes[0].plot(mean_trajectory, label = \"mean\")\n",
    "    axes[0].plot(mean_trajectory + std_trajectory, color = \"red\", label = \"+- std\")\n",
    "    axes[0].plot(mean_trajectory - std_trajectory, color = \"red\")\n",
    "    axes[0].set_xlabel(\"Optimization Step\")\n",
    "    axes[0].set_ylabel(\"Drift b\")\n",
    "    value_trajectory = sde_dict[\"value_trajectory\"].detach()\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_trajectory = torch.mean(value_trajectory, dim = 1)\n",
    "    std_trajectory = torch.std(value_trajectory, dim = 1)\n",
    "    ## defining figure and axis\n",
    "    axes[1].set_title(\"Simulated Value trajectories\")\n",
    "    axes[1].plot(value_trajectory[:, :, 0])\n",
    "    axes[1].set_xlabel(\"Time Step\")\n",
    "    axes[1].set_ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing apf-filtering config dictionary\n",
    "apf_config = {\n",
    "    \"num_time_steps\": 100,\n",
    "    \"num_particles\": 2**9,\n",
    "    \"num_obs\": 100\n",
    "}\n",
    "\n",
    "## filter from initial state and full sequence of observations\n",
    "observation_idx = 0\n",
    "num_obs = 1000\n",
    "num_particles = 2**4\n",
    "## retrieving initial state for target sequence\n",
    "x0 = experiment.ssm.test_sim[\"latent_states\"][0, observation_idx, :]\n",
    "x0 = torch.unsqueeze(x0, dim = 0)\n",
    "x0 = x0.repeat((apf_config[\"num_particles\"], 1))\n",
    "## retrieving the whole sequence of observations\n",
    "y = experiment.ssm.test_sim[\"observations\"][:apf_config[\"num_obs\"], observation_idx, :]\n",
    "## retrieving ground truth sequence\n",
    "x_gt = experiment.ssm.test_sim[\"latent_states\"][:apf_config[\"num_obs\"], observation_idx, :]\n",
    "## constructing the batch\n",
    "batch = {\n",
    "    \"x0\": x0,\n",
    "    \"xc\": x0,\n",
    "    \"y\": y\n",
    "}\n",
    "batch = move_batch_to_device(batch, experiment.device)\n",
    "for key, tensor in batch.items():\n",
    "    print(key, tensor.shape)\n",
    "## running filtering\n",
    "apf_dict = experiment.FA_APF(batch, apf_config)\n",
    "## displaying filtering metrics\n",
    "print(torch.mean(apf_dict[\"ess\"]))\n",
    "print(apf_dict[\"log_norm_const\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting elbo and ess\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axes[0].plot(apf_dict[\"log_norm_const\"])\n",
    "axes[0].set_title(\"Log Observation Likelihood\")\n",
    "axes[1].plot(apf_dict[\"ess\"])\n",
    "axes[1].set_title(\"Expected Sample Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting filtered states\n",
    "filtered_states = apf_dict[\"states\"]\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(x_gt, label = \"GT latent states\", color = \"red\")\n",
    "#axes.plot(filtered_states[:, :, 0].T, color = \"purple\", label = \"filtered latent states\")\n",
    "axes.plot(torch.mean(filtered_states[:, :, 0], dim = 0), label = \"filtered latent states\", color = \"purple\")\n",
    "axes.set_title(\"Filtered vs GT States\")\n",
    "legend = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## autoregressive sampling\n",
    "AR_SAMPLING = True\n",
    "if AR_SAMPLING:\n",
    "    ## constructing autoregressive sampling config dictionary\n",
    "    ar_sample_config = {\n",
    "        \"num_time_steps\": config[\"num_time_steps\"],\n",
    "        \"num_ar_steps\": 100,#config[\"num_ar_steps\"],\n",
    "        \"initial_time_step\": config[\"initial_time_step\"],\n",
    "        \"ar_sample_train\": config[\"ar_sample_train\"],\n",
    "    }\n",
    "    ## getting sample batch\n",
    "    batch = experiment.get_batch(train = False, idx = config[\"initial_time_step\"])\n",
    "    batch = move_batch_to_device(batch, experiment.device) \n",
    "    ## sampling from model\n",
    "    sample_dict = experiment.ar_sample(batch, config = ar_sample_config)\n",
    "    ## parsing samples dict\n",
    "    ar_samples = sample_dict[\"ar_samples\"]\n",
    "    ## displaying the shape of the results\n",
    "    print(f\"{ar_samples.shape=}\", end = \"\")\n",
    "    if config[\"full_out\"]:\n",
    "        trajectory = sample_dict[\"trajectory\"]\n",
    "        drift = sample_dict[\"drift\"]\n",
    "        diffusion = sample_dict[\"diffusion\"]\n",
    "        ## displaying the shape of the results\n",
    "        print(f\", {trajectory.shape=}, {drift.shape=}, {diffusion.shape=}\")\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_ar_samples = torch.mean(ar_samples, dim = (0, 2))\n",
    "    std_ar_samples = torch.std(ar_samples, dim = (0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting example trajectory\n",
    "gt_trajectory = experiment.ssm.test_sim[\"latent_states\"][:ar_sample_config[\"num_ar_steps\"],:,:]\n",
    "observation_ids = np.random.randint(args[\"num_sims\"], size = (10))\n",
    "#observation_idx = 200\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "fig.suptitle(f\"AR vs GT States Simulation id: {observation_ids}\")\n",
    "for idx in range(10):\n",
    "    r_id = idx//5\n",
    "    c_id = idx%5\n",
    "    axes[r_id, c_id].plot(ar_samples[:, observation_ids[idx], ], label = \"Ar samples\", color = \"purple\")\n",
    "    axes[r_id, c_id].plot(gt_trajectory[:,observation_ids[idx], ], label = \"GT trajectory\", color = \"red\")\n",
    "    #axes.plot(ar_samples[:, :, 0], label = \"Ar samples\", color = \"purple\")\n",
    "    #axes.plot(gt_trajectory[:, :, 0], label = \"GT trajectory\", color = \"red\")\n",
    "    #legend = plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
