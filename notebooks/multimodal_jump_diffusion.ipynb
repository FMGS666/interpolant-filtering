{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os \n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.src import (\n",
    "    create_interpolant,\n",
    "    create_models,\n",
    "    IdentityPreproc,\n",
    "    StandardizeSim,\n",
    "    SimSSM,\n",
    "    DriftObjective,\n",
    ")\n",
    "\n",
    "from int_filt.utils import (\n",
    "    InputData, \n",
    "    OutputData,\n",
    "    ConfigData,\n",
    "    configuration,\n",
    "    ensure_reproducibility, \n",
    "    move_batch_to_device,\n",
    "    dump_config,\n",
    "    construct_time_discretization,\n",
    ")\n",
    "\n",
    "from int_filt.experiments import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting reproducibility\n",
    "reproducible = True\n",
    "SEED = 1024 if reproducible else int(time.time())\n",
    "ensure_reproducibility(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder = \"../archive/data/multi_modal_jump_diffusion\"\n",
    "## globals\n",
    "PREPROCESSING = {\n",
    "    \"none\": IdentityPreproc,\n",
    "    \"sim\": StandardizeSim,\n",
    "}\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"sgd\": torch.optim.SGD,\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None,\n",
    "    \"cosine-annealing\": torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## objects and functions for handling experiment\n",
    "## defining sampling function\n",
    "def pair_lagged_observations(observation_store, lag):\n",
    "    num_observations = observation_store.shape[0]\n",
    "    indexes = torch.arange(num_observations - lag)\n",
    "    current_observation = observation_store[indexes]\n",
    "    next_observation = observation_store[indexes + 1]\n",
    "    return current_observation, next_observation\n",
    "\n",
    "## defining class for multimodal jump diffusion ssm\n",
    "class LoadStoredSSM(SimSSM):\n",
    "    def __init__(self, config: ConfigData):\n",
    "        self.train_sim = config[\"train_sim\"]\n",
    "        self.test_sim = config[\"test_sim\"]\n",
    "\n",
    "# class for handling the paired lagged datasets\n",
    "class LaggedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, current_states, next_states, device):\n",
    "        super(LaggedDataset, self).__init__()\n",
    "        self.current_states = current_states\n",
    "        self.next_states = next_states\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.next_states.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x0 = self.current_states[idx].to(self.device)\n",
    "        x1 = self.next_states[idx].to(self.device)\n",
    "        batch_dict = {\"x0\": x0, \"xc\": x0, \"x1\": x1, \"y\": x1}\n",
    "        return batch_dict\n",
    "\n",
    "## defining class for handling experiments\n",
    "class MultimodalJumpDiffusion(Experiment):\n",
    "    def __init__(self, config):\n",
    "        super(MultimodalJumpDiffusion, self).__init__(config)\n",
    "        self.N = 1000\n",
    "\n",
    "    def train_drift(self, config: ConfigData) -> OutputData:\n",
    "        \"\"\"\n",
    "        Trains the $b$ model\n",
    "        \"\"\"\n",
    "        ## retrieving data loader\n",
    "        data_loader = config[\"data_loader\"]\n",
    "        ## retrieving optimizer and scheduler\n",
    "        optimizer = config[\"b_net_optimizer\"]\n",
    "        scheduler = config[\"b_net_scheduler\"]\n",
    "        ## initializing objective function\n",
    "        objective_config = {\n",
    "            \"b_net\": self.b_net, \n",
    "            \"interpolant\": self.interpolant, \n",
    "            \"mc_config\": self.mc_config,\n",
    "            \"preprocessing\": self.preprocessing,\n",
    "        }\n",
    "        objective = DriftObjective(objective_config)\n",
    "        ## allocating memory for storing loss and lr\n",
    "        loss_history = torch.zeros((config[\"num_grad_steps\"]))\n",
    "        lr_history = torch.zeros((config[\"num_grad_steps\"]))\n",
    "        ## starting optimization\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            ## defining iterator\n",
    "            iterator = tqdm(range(len(data_loader)))\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                ## preparing batch\n",
    "                batch = move_batch_to_device(batch, self.device)\n",
    "                ## estimating loss\n",
    "                loss_dict = objective.forward(batch)\n",
    "                # parsing loss dictionary\n",
    "                loss = loss_dict[\"loss\"]\n",
    "                ## retrieving loss value\n",
    "                loss_value = loss.item()\n",
    "                ## optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                ## scheduler step \n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                # retrieving learning rate\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                ## progress bar\n",
    "                msg = f\"MSELoss: {loss_value}, Learning Rate {current_lr}\"\n",
    "                iterator.set_description(msg)\n",
    "                iterator.update()\n",
    "                ## storing loss and lr and sampled drifts\n",
    "                current_step = epoch*len(data_loader) + idx\n",
    "                loss_history[current_step] = loss_value\n",
    "                lr_history[current_step] = current_lr\n",
    "                ## cleaning up memory\n",
    "                if self.clear_memory:\n",
    "                    del batch, loss_dict \n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "        ## constructing output dictionary\n",
    "        train_dict = {\"loss_history\": loss_history, \"lr_history\": lr_history}\n",
    "        return train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training states and observations\n",
    "train_state_store = torch.load(os.path.join(data_folder, \"train_states.pt\"))\n",
    "train_observation_store = torch.load(os.path.join(data_folder, \"train_observations.pt\"))\n",
    "# loading testing states and observations\n",
    "test_state_store = torch.load(os.path.join(data_folder, \"test_states.pt\"))\n",
    "test_observation_store = torch.load(os.path.join(data_folder, \"test_observations.pt\"))\n",
    "# defining train simulation dictionary (no perturbation just between observation lag)\n",
    "train_sim = {\"latent_states\": train_observation_store, \"observations\": train_observation_store}\n",
    "test_sim = {\"latent_states\": train_observation_store, \"observations\": test_observation_store}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining experiment config\n",
    "experiment_config = {\n",
    "    ## interpolant config\n",
    "    \"epsilon\": 1e-0,\n",
    "    \"interpolant_method\": \"pffp_v0\",\n",
    "    ## model config\n",
    "    \"b_net_amortized\": False,\n",
    "    \"b_net_activation\": \"relu\",\n",
    "    \"b_net_hidden_dims\": [500]*5,\n",
    "    \"b_net_activate_final\": False,\n",
    "    ## training config\n",
    "    \"b_net_lr\": 1e-3,\n",
    "    \"num_grad_steps\": 1000,\n",
    "    \"b_net_scheduler\": \"cosine-annealing\",\n",
    "    \"b_net_optimizer\": \"adam-w\",\n",
    "    ## mc estimation config\n",
    "    \"num_mc_samples\": 300,\n",
    "    ## preprocessing config\n",
    "    \"preprocessing\": \"none\",\n",
    "    \"postprocessing\": False,\n",
    "    ## ssm config\n",
    "    \"num_dims\": 2,\n",
    "    ## memory management config\n",
    "    \"clear_memory\": True\n",
    "}\n",
    "\n",
    "## parsing default arguments\n",
    "config = configuration(args=[])\n",
    "\n",
    "## creating experiment\n",
    "config = vars(config)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_config.items():\n",
    "    config[k] = v\n",
    "\n",
    "## displaying current settings\n",
    "print(config)\n",
    "\n",
    "## retrieving activations\n",
    "config[\"b_net_activation\"] = ACTIVATIONS[config[\"b_net_activation\"]]\n",
    "## retrieving device\n",
    "config[\"device\"] = DEVICES[config[\"device\"]]\n",
    "\n",
    "## adding mc configuration\n",
    "config[\"mc_config\"] = {\"num_mc_samples\": config[\"num_mc_samples\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing interpolant\n",
    "interpolant_config = {\"method\": config[\"interpolant_method\"], \"epsilon\": config[\"epsilon\"]}\n",
    "interpolant = create_interpolant(interpolant_config)\n",
    "## initializing models\n",
    "models_config = {\n",
    "    \"backbone\": config[\"backbone\"],\n",
    "    \"spatial_dims\": config[\"num_dims\"],\n",
    "    \"b_net_hidden_dims\": config[\"b_net_hidden_dims\"],\n",
    "    \"b_net_activation\": config[\"b_net_activation\"],\n",
    "    \"b_net_activate_final\": config[\"b_net_activate_final\"],\n",
    "    \"b_net_amortized\": config[\"b_net_amortized\"],\n",
    "    \"device\": config[\"device\"]\n",
    "}\n",
    "models = create_models(models_config)\n",
    "b_net = models[\"b_net\"]\n",
    "print(b_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing state space model configuration\n",
    "ssm_config = {\n",
    "    \"train_sim\": train_sim,\n",
    "    \"test_sim\": test_sim\n",
    "}\n",
    "\n",
    "## initializing state space model\n",
    "ssm = LoadStoredSSM(ssm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing preprocessing\n",
    "preprocessing_config = {\n",
    "    \"ssm\": ssm\n",
    "}\n",
    "preprocessing = PREPROCESSING[config[\"preprocessing\"]](preprocessing_config)\n",
    "\n",
    "## logging \n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing experiment\n",
    "experiment_config = {\n",
    "    \"interpolant\": interpolant,\n",
    "    \"b_net\": b_net, \n",
    "    \"ssm\": ssm,\n",
    "    \"preprocessing\": preprocessing,\n",
    "    \"writer\": writer,\n",
    "    \"postprocessing\": config[\"postprocessing\"],\n",
    "    \"log_results\": config[\"log_results\"], \n",
    "    \"logging_step\": config[\"logging_step\"],\n",
    "    \"mc_config\": config[\"mc_config\"],\n",
    "    \"device\": config[\"device\"],\n",
    "    \"full_out\": config[\"full_out\"],\n",
    "    \"clear_memory\": config[\"clear_memory\"],\n",
    "}\n",
    "\n",
    "experiment = MultimodalJumpDiffusion(experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare for training\n",
    "## initializing data loader\n",
    "X_train, Y_train = pair_lagged_observations(train_observation_store, 1)\n",
    "train_dataset = LaggedDataset(X_train, Y_train, device = device)\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1000, shuffle = True)\n",
    "\n",
    "## initializing optimizer and scheduler\n",
    "b_net_optimizer = OPTIMIZERS[config[\"b_net_optimizer\"]](experiment.b_net.backbone.parameters(), lr = config[\"b_net_lr\"])\n",
    "b_net_scheduler = SCHEDULERS[config[\"b_net_scheduler\"]]\n",
    "if b_net_scheduler is not None:\n",
    "    b_net_scheduler = b_net_scheduler(b_net_optimizer, config[\"num_grad_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting example batch\n",
    "data_loader_ = torch.utils.data.DataLoader(train_dataset, batch_size = 1000, shuffle = False)\n",
    "batch = next(iter(data_loader_))\n",
    "print(batch)\n",
    "batch[\"xt\"] = batch[\"x0\"]\n",
    "batch[\"xc\"] = batch[\"xc\"]\n",
    "batch[\"t\"] = torch.zeros((experiment.N), device = device)\n",
    "out = b_net(batch)\n",
    "print(out)\n",
    "print(f\"{out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardization\n",
    "if getattr(experiment.preprocessing, \"params\", False):\n",
    "    print(f\"STANDARDIZATION: {experiment.preprocessing.params}\")\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing.standardize(batch)\n",
    "print(\"\\nAFTER PREPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing.unstandardize(batch)\n",
    "print(\"\\nAFTER POSTPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"data_loader\": data_loader,\n",
    "    \"num_epochs\": 10,\n",
    "    \"b_net_optimizer\": b_net_optimizer,\n",
    "    \"b_net_scheduler\": b_net_scheduler,\n",
    "    \"num_grad_steps\": 10*len(data_loader),\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "train_dict = experiment.train_drift(b_net_optim_config)\n",
    "loss_history = train_dict[\"loss_history\"]\n",
    "lr_history = train_dict[\"lr_history\"]\n",
    "## optional logging\n",
    "if config[\"log_results\"]:\n",
    "    ## saving the model\n",
    "    torch.save(experiment.b_net.state_dict(), os.path.join(config[\"dump_dir\"], \"b_net.pt\"))\n",
    "## displaying the shape of the results\n",
    "print(f\"{loss_history.shape=}, {lr_history.shape=}\")\n",
    "## plotting loss and lr history\n",
    "fix, axes = plt.subplots(1, 2)\n",
    "## plotting loss \n",
    "axes[0].plot(loss_history)\n",
    "axes[0].set_title(\"Loss History\")\n",
    "axes[0].set_xlabel(\"Gradient Step\")\n",
    "axes[0].set_ylabel(\"Loss Value\")\n",
    "## plotting learning rate\n",
    "axes[1].plot(lr_history)\n",
    "axes[1].set_title(\"Learning Rate History\")\n",
    "axes[1].set_xlabel(\"Gradient Step\")\n",
    "axes[1].set_ylabel(\"Learning Rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
