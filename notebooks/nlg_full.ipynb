{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils.config import configuration\n",
    "from int_filt.utils.utils import ensure_reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}\n",
    "\n",
    "## defining settings\n",
    "num_dims = 1\n",
    "b_net_amortized = True\n",
    "sigma_x = 1e-2\n",
    "sigma_y = 1e-2\n",
    "\n",
    "## plotting\n",
    "num_observations_to_plot = 1000\n",
    "num_iters_to_plot = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"cos\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_cos = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_cos = torch.squeeze(experiment_nlg_cos.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_cos = torch.squeeze(experiment_nlg_cos.ssm.sim[\"latent_states\"]).numpy().T\n",
    "print(f\"{latent_states_nlg_cos.shape=}, {observations_nlg_cos.shape=}\")\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_cos = pd.DataFrame(latent_states_nlg_cos[ : num_observations_to_plot, : num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "#fig, axes = joypy.joyplot(latent_states_nlg_cos, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_cos.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_cos.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_cos.b_net.state_dict(), os.path.join(dump_dir, \"b_net_cos.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"exp\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_exp = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_exp = torch.squeeze(experiment_nlg_exp.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_exp.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_exp.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_exp.b_net.state_dict(), os.path.join(dump_dir, \"b_net_exp.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"sin\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_sin = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_sin = torch.squeeze(experiment_nlg_sin.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_sin = torch.squeeze(experiment_nlg_sin.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_sin = pd.DataFrame(latent_states_nlg_sin[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "fig, axes = joypy.joyplot(latent_states_nlg_sin, ylabels=False, xlabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_sin.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_sin.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_sin.b_net.state_dict(), os.path.join(dump_dir, \"b_net_sin.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tangent non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"tan\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_tan = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_tan = torch.squeeze(experiment_nlg_tan.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_tan = torch.squeeze(experiment_nlg_tan.ssm.sim[\"latent_states\"]).numpy().T\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_tan = pd.DataFrame(latent_states_nlg_tan[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "fig, axes = joypy.joyplot(latent_states_nlg_sin, ylabels=False, xlabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_tan.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_tan.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_tan.b_net.state_dict(), os.path.join(dump_dir, \"b_net_tan.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperbolic Tangent non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "experiment_conf = {\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"tanh\",\n",
    "    \"num_dims\": num_dims,\n",
    "    \"b_net_amortized\": b_net_amortized,\n",
    "    \"sigma_x\": sigma_x,\n",
    "    \"sigma_y\": sigma_y\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "## retrieving activations\n",
    "args.b_net_activation = ACTIVATIONS[args.b_net_activation]\n",
    "## retrieving device\n",
    "args.device = DEVICES[args.device]\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_conf.items():\n",
    "    args[k] = v\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_samples\": args[\"num_samples\"]}\n",
    "\n",
    "## prepare for training drift\n",
    "b_net_num_grad_step = args[\"b_net_num_grad_steps\"]\n",
    "b_net_optimizer = args[\"b_net_optimizer\"]\n",
    "b_net_scheduler = args[\"b_net_scheduler\"]\n",
    "b_net_lr = args[\"b_net_lr\"]\n",
    "\n",
    "## dump dir \n",
    "dump_dir = args[\"dump_dir\"]\n",
    "path = Path(dump_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## reproducibility\n",
    "random_seed = args[\"random_seed\"]\n",
    "ensure_reproducibility(random_seed)\n",
    "\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment_nlg_tanh = create_experiment(args)\n",
    "\n",
    "## joyplot\n",
    "## retrieving data\n",
    "latent_states_nlg_tanh = torch.squeeze(experiment_nlg_tanh.ssm.sim[\"latent_states\"]).numpy().T\n",
    "observations_nlg_tanh = torch.squeeze(experiment_nlg_tanh.ssm.sim[\"latent_states\"]).numpy().T\n",
    "print(f\"{latent_states_nlg_tanh.shape=}, {observations_nlg_tanh.shape=}\")\n",
    "## constructing data frame\n",
    "#observation_indices = np.arange(args[\"num_iters\"])\n",
    "observation_indices = np.arange(num_iters_to_plot)\n",
    "latent_states_nlg_tanh = pd.DataFrame(latent_states_nlg_tanh[ : num_observations_to_plot, : num_iters_to_plot], columns = observation_indices)\n",
    "## ridge plot\n",
    "fig, axes = joypy.joyplot(latent_states_nlg_tanh, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing optimizer\n",
    "b_net_optimizer = OPTIMIZERS[b_net_optimizer](experiment_nlg_tanh.b_net.backbone.parameters(), lr = b_net_lr)\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"num_grad_steps\": b_net_num_grad_step,\n",
    "    \"optimizer\": b_net_optimizer,\n",
    "    \"scheduler\": b_net_scheduler\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "experiment_nlg_tanh.train(b_net_optim_config)\n",
    "## saving the weights\n",
    "torch.save(experiment_nlg_tanh.b_net.state_dict(), os.path.join(dump_dir, \"b_net_tanh.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
