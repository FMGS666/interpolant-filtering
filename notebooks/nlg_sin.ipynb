{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import joypy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## exposing path \n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from int_filt.experiments import create_experiment\n",
    "from int_filt.utils import (\n",
    "    configuration,\n",
    "    ensure_reproducibility, \n",
    "    move_batch_to_device,\n",
    "    dump_config,\n",
    "    construct_time_discretization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## globals\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": torch.nn.ReLU()\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"sgd\": torch.optim.SGD,\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adam-w\": torch.optim.AdamW\n",
    "}\n",
    "\n",
    "SCHEDULERS = {\n",
    "    \"none\": None,\n",
    "    \"cosine-annealing\": torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "}\n",
    "\n",
    "DEVICES = {\n",
    "    \"cpu\": torch.device(\"cpu\"),\n",
    "    \"cuda\": torch.device(\"cuda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining configurations\n",
    "DEBUG = False\n",
    "experiment_config = {\n",
    "    ## simulation config\n",
    "    \"experiment\": \"nlg\",\n",
    "    \"non_linearity\": \"sin\",\n",
    "    \"num_dims\": 1,\n",
    "    \"num_sims\": 1_000,\n",
    "    \"num_iters\": 1_000 if DEBUG else 1_000_000,\n",
    "    \"num_burn_in_steps\": 0,\n",
    "    \"step_size\": 1e-3,\n",
    "    \"sigma_x\": 1e-2,\n",
    "    \"sigma_y\": 1e-2,\n",
    "    \"beta\": 1e-0,\n",
    "    ## interpolant config\n",
    "    \"epsilon\": 0.5e-1,\n",
    "    \"interpolant_method\": \"pffp_v0\",\n",
    "    ## model config\n",
    "    \"b_net_amortized\": False,\n",
    "    \"b_net_activation\": \"relu\",\n",
    "    \"b_net_hidden_dims\": [500]*5,\n",
    "    \"b_net_activate_final\": False,\n",
    "    ## training config\n",
    "    \"b_net_lr\": 1e-3,\n",
    "    \"num_grad_steps\": 1 if DEBUG else 100,\n",
    "    \"b_net_scheduler\": \"none\",\n",
    "    \"b_net_optimizer\": \"adam-w\",\n",
    "    ## mc estimation config\n",
    "    \"num_mc_samples\": 10 if DEBUG else 300,\n",
    "    ## preprocessing config\n",
    "    \"preprocessing\": \"sim\",\n",
    "    \"postprocessing\": True,\n",
    "    ## sampling config\n",
    "    \"num_samples\": 10 if DEBUG else 250,\n",
    "    \"num_time_steps\": 10 if DEBUG else 100,\n",
    "    \"num_ar_steps\": 10 if DEBUG else 1_000,\n",
    "    \"full_out\": True,\n",
    "    \"initial_time_step\": 0,\n",
    "    \"ar_sample_train\": False,\n",
    "    ## logging config\n",
    "    \"log_results\": False,\n",
    "    ## memory management config\n",
    "    \"clear_memory\": True\n",
    "}\n",
    "## parsing default arguments\n",
    "args = configuration(args=[])\n",
    "\n",
    "## creating experiment\n",
    "args = vars(args)\n",
    "\n",
    "## setting current configurations\n",
    "for k, v in experiment_config.items():\n",
    "    args[k] = v\n",
    "\n",
    "## optional logging\n",
    "if args[\"log_results\"]:\n",
    "    ## creating dump dir \n",
    "    path = Path(args[\"dump_dir\"])\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    dump_config(args, os.path.join(args[\"dump_dir\"], \"config.json\"))\n",
    "\n",
    "## retrieving activations\n",
    "args[\"b_net_activation\"] = ACTIVATIONS[args[\"b_net_activation\"]]\n",
    "## retrieving device\n",
    "args[\"device\"] = DEVICES[args[\"device\"]]\n",
    "\n",
    "## adding mc configuration\n",
    "args[\"mc_config\"] = {\"num_mc_samples\": args[\"num_mc_samples\"]}\n",
    "\n",
    "## reproducibility\n",
    "ensure_reproducibility(args[\"random_seed\"])\n",
    "\n",
    "## displaying current arguments\n",
    "print(args)\n",
    "\n",
    "## creating experiment\n",
    "experiment = create_experiment(args)\n",
    "## displaying simulations shape\n",
    "print(f\"{experiment.ssm.train_sim[\"latent_states\"].shape}, {experiment.ssm.train_sim[\"observations\"].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting simulation \n",
    "PLOT_SIMULATION = False\n",
    "JOYPLOT = False\n",
    "if PLOT_SIMULATION:\n",
    "    ## defining number of trajectories and time steps to plot\n",
    "    num_trajectories = 200\n",
    "    num_time_steps = 10000\n",
    "    ## retrieving data\n",
    "    latent_states = torch.squeeze(experiment.ssm.train_sim[\"latent_states\"])[:num_time_steps, :num_trajectories]\n",
    "    observations = torch.squeeze(experiment.ssm.train_sim[\"latent_states\"])[:num_time_steps, :num_trajectories]\n",
    "    ## computing statistics of states\n",
    "    latent_states_mean = torch.mean(latent_states, dim = 1)\n",
    "    latent_states_std = torch.std(latent_states, dim = 1)\n",
    "    ## computing statistics of states\n",
    "    observations_mean = torch.mean(observations, dim = 1)\n",
    "    observations_std = torch.std(observations, dim = 1)\n",
    "    ## plotting statistics over time\n",
    "    fig, axes = plt.subplots()\n",
    "    ## plotting latent states\n",
    "    axes.plot(latent_states, color = \"purple\")\n",
    "    axes.plot(latent_states_mean)\n",
    "    axes.plot(latent_states_mean + latent_states_std, color = \"red\")\n",
    "    axes.plot(latent_states_mean - latent_states_std, color = \"red\")\n",
    "    if args[\"log_results\"]:\n",
    "        experiment.writer.add_figure(\"simulated ssm\", fig)\n",
    "        \n",
    "if JOYPLOT:\n",
    "    ## retrieving data\n",
    "    latent_states = torch.squeeze(experiment.ssm.train_sim[\"latent_states\"]).numpy().T\n",
    "    observations_nlg_exp = torch.squeeze(experiment.ssm.train_sim[\"latent_states\"]).numpy().T\n",
    "    ## constructing data frame\n",
    "    observation_indices = np.arange(args[\"num_iters\"])\n",
    "    observation_indices = np.arange(num_iters_to_plot)\n",
    "    latent_states_nlg_exp = pd.DataFrame(latent_states_nlg_exp[:num_observations_to_plot, :num_iters_to_plot], columns = observation_indices)\n",
    "    ## ridge plot\n",
    "    fig, axes = joypy.joyplot(latent_states_nlg_exp, ylabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting interpolant\n",
    "PLOT_INTERPOLANT = True\n",
    "if PLOT_INTERPOLANT:\n",
    "    ## visualizing interpolant trajectory\n",
    "    ## getting sample batch\n",
    "    batch = experiment.get_batch(train = False)\n",
    "    batch = move_batch_to_device(batch, experiment.device)\n",
    "    batch = experiment.preprocessing.standardize(batch)\n",
    "    ## printing shape\n",
    "    for key, tensor in batch.items():\n",
    "        print(key, tensor.shape)\n",
    "    ## parsing batch dictionary\n",
    "    x0 = batch[\"x0\"]\n",
    "    x1 = batch[\"x1\"]\n",
    "    xc = batch[\"xc\"]\n",
    "    y = batch[\"y\"]\n",
    "    ## dscretizing the time interval\n",
    "    time, stepsizes = construct_time_discretization(args[\"num_time_steps\"], device = experiment.device)\n",
    "    ## allocating memory\n",
    "    interpolant_trajectory = torch.zeros((args[\"num_time_steps\"] + 1, args[\"num_sims\"], args[\"num_dims\"]), device = experiment.device)\n",
    "    ## sampling noise\n",
    "    noise = torch.randn((args[\"num_time_steps\"] + 1, args[\"num_sims\"], args[\"num_dims\"]), device = experiment.device)\n",
    "    ## definig iterator\n",
    "    iterator = tqdm(range(args[\"num_time_steps\"] + 1))\n",
    "    for s in iterator:\n",
    "        ## retrieving current terms\n",
    "        t = time[s]*torch.ones((args[\"num_sims\"], args[\"num_dims\"]), device = experiment.device)\n",
    "        z = noise[s]\n",
    "        ## constructing batch\n",
    "        mc_batch = {\"t\": t, \"x0\": x0, \"x1\": x1, \"xc\": xc, \"z\": z, \"y\": y}\n",
    "        ## computing interpolant\n",
    "        xs = experiment.interpolant.interpolant(mc_batch)\n",
    "        ## storing result\n",
    "        interpolant_trajectory[s] = xs\n",
    "    ## plotting trajectory of interpolant\n",
    "    print(interpolant_trajectory.shape)\n",
    "    plt.plot(interpolant_trajectory[:, :, 0].cpu())\n",
    "    ## asserting equality\n",
    "    x1 = torch.squeeze(x1).cpu()\n",
    "    xt = torch.squeeze(interpolant_trajectory[-1]).cpu()\n",
    "    print(torch.equal(x1, xt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardization\n",
    "batch = experiment.get_batch()\n",
    "if getattr(experiment.preprocessing, \"params\", None) is not None:\n",
    "    print(f\"STANDARDIZATION: {experiment.preprocessing.params}\")\n",
    "print(\"BEFORE STANDARDIZATION\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing.standardize(batch)\n",
    "print(\"\\nAFTER PREPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)\n",
    "batch = experiment.preprocessing.unstandardize(batch)\n",
    "print(\"\\nAFTER POSTPROCESSING\\n\")\n",
    "for k, v in batch.items():\n",
    "    print(k, \"-> mean: \", v.mean(), \", std: \", v.std(), \"shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "## initializing optimizer and scheduler\n",
    "b_net_optimizer = OPTIMIZERS[args[\"b_net_optimizer\"]](experiment.b_net.backbone.parameters(), lr = args[\"b_net_lr\"])\n",
    "b_net_scheduler = SCHEDULERS[args[\"b_net_scheduler\"]]\n",
    "if b_net_scheduler is not None:\n",
    "    b_net_scheduler = b_net_scheduler(b_net_optimizer, args[\"num_grad_steps\"])\n",
    "\n",
    "## constructing optimization config dictionary\n",
    "b_net_optim_config = {\n",
    "    \"b_net_optimizer\": b_net_optimizer,\n",
    "    \"b_net_scheduler\": b_net_scheduler,\n",
    "    \"num_grad_steps\": args[\"num_grad_steps\"],\n",
    "}\n",
    "\n",
    "## training b_net \n",
    "train_dict = experiment.train_drift(b_net_optim_config)\n",
    "loss_history = train_dict[\"loss_history\"]\n",
    "lr_history = train_dict[\"lr_history\"]\n",
    "## optional logging\n",
    "if args[\"log_results\"]:\n",
    "    ## saving the model\n",
    "    torch.save(experiment.b_net.state_dict(), os.path.join(args[\"dump_dir\"], \"b_net.pt\"))\n",
    "## displaying the shape of the results\n",
    "print(f\"{loss_history.shape=}, {lr_history.shape=}\")\n",
    "## plotting loss and lr history\n",
    "fix, axes = plt.subplots(1, 2)\n",
    "## plotting loss \n",
    "axes[0].plot(loss_history)\n",
    "axes[0].set_title(\"Loss History\")\n",
    "axes[0].set_xlabel(\"Gradient Step\")\n",
    "axes[0].set_ylabel(\"Loss Value\")\n",
    "## plotting learning rate\n",
    "axes[1].plot(lr_history)\n",
    "axes[1].set_title(\"Learning Rate History\")\n",
    "axes[1].set_xlabel(\"Gradient Step\")\n",
    "axes[1].set_ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sde_config = {\n",
    "    \"num_time_steps\": 50, #args[\"num_time_steps\"],\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment.get_batch(train = False)\n",
    "batch = move_batch_to_device(batch, experiment.device)\n",
    "\n",
    "## simulating sde on sample batch\n",
    "sde_dict = experiment.simulate_sde(batch, config = sde_config)\n",
    "## displaying results shape\n",
    "for key, tensor in sde_dict.items():\n",
    "    print(key, f\": {tensor.shape}\")\n",
    "## plotting aggregated drift history over optimization\n",
    "if args[\"full_out\"]:\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_trajectory = torch.mean(sde_dict[\"trajectory\"], dim = 1)\n",
    "    std_trajectory = torch.std(sde_dict[\"trajectory\"], dim = 1)\n",
    "    ## defining figure and axis\n",
    "    #fig, axes = plt.subplots()\n",
    "    #axes.set_title(\"Simulated trajectories\")\n",
    "    #axes.plot(sde_dict[\"trajectory\"][:, :, 0])\n",
    "    #axes.plot(mean_trajectory, label = \"mean\")\n",
    "    #axes.plot(mean_trajectory + std_trajectory, color = \"red\", label = \"+- std\")\n",
    "    #axes.plot(mean_trajectory - std_trajectory, color = \"red\")\n",
    "    #axes.set_xlabel(\"Time Step\")\n",
    "    #axes.set_ylabel(\"Trajectory\")\n",
    "    #legend = plt.legend()\n",
    "    ## plotting sampled trajectory\n",
    "    plt.plot(sde_dict[\"trajectory\"][:, 0, 0])\n",
    "    plt.axhline(batch[\"x0\"][0, 0].cpu())\n",
    "    plt.axhline(batch[\"x1\"][0, 0].cpu(), color = \"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing sampling config dictionary\n",
    "sample_config = {\n",
    "    \"num_time_steps\": args[\"num_time_steps\"],\n",
    "    \"num_samples\": args[\"num_samples\"]\n",
    "}\n",
    "\n",
    "## getting sample batch\n",
    "batch = experiment.get_batch(train = False)\n",
    "batch = move_batch_to_device(batch, experiment.device)\n",
    "\n",
    "## sampling from model\n",
    "sample_dict = experiment.sample(batch, config = sample_config)\n",
    "## parsing samples dict\n",
    "samples = sample_dict[\"samples\"]\n",
    "if args[\"full_out\"]:\n",
    "    trajectory = sample_dict[\"trajectory\"]\n",
    "\n",
    "## sampling from gt state transition\n",
    "x = batch[\"x0\"]\n",
    "samples_gt = torch.zeros_like(samples)\n",
    "## defining progress bar\n",
    "iterator = tqdm(range(args[\"num_samples\"]))\n",
    "for sample_id in iterator:\n",
    "    x1 = experiment.ssm.state_transition(x)\n",
    "    samples_gt[sample_id] = x1\n",
    "## displaying the shape of the results\n",
    "print(f\"{samples.shape=}\", end = \"\") \n",
    "if args[\"full_out\"]:\n",
    "    print(f\", {trajectory.shape=}, {samples_gt.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting example histogram\n",
    "observation_idx = 100\n",
    "gt_dist = samples_gt[:, observation_idx, 0]\n",
    "predicted_dist = samples[:, observation_idx, 0]\n",
    "print(f\"{gt_dist.shape=}, {predicted_dist.shape}\")\n",
    "print(f\"{gt_dist.mean()=}, {predicted_dist.mean()=}\")\n",
    "print(f\"{gt_dist.std()=}, {predicted_dist.std()=}\")\n",
    "fig, axes = plt.subplots()\n",
    "axes = sns.kdeplot(gt_dist, label = \"ground truth\")\n",
    "axes = sns.kdeplot(predicted_dist, label = \"prediction\")\n",
    "## plotting preceeding state\n",
    "x = batch[\"x0\"]\n",
    "x1 = experiment.ssm.non_linearity(x)\n",
    "print(f\"{x[observation_idx,:]=}, {x1[observation_idx, :]} {x1[observation_idx, :] - x[observation_idx, :]}\")\n",
    "## displaying legends\n",
    "plt.legend()\n",
    "if args[\"log_results\"]:\n",
    "    experiment.writer.add_figure(\"simulations/learned_distribution\", fig)\n",
    "else:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## autoregressive sampling\n",
    "AR_SAMPLING = True\n",
    "if AR_SAMPLING:\n",
    "    ## constructing autoregressive sampling config dictionary\n",
    "    ar_sample_config = {\n",
    "        \"num_time_steps\": args[\"num_time_steps\"],\n",
    "        #\"num_ar_steps\": args[\"num_ar_steps\"],\n",
    "        \"num_ar_steps\": 100,\n",
    "        \"initial_time_step\": args[\"initial_time_step\"],\n",
    "        \"ar_sample_train\": args[\"ar_sample_train\"],\n",
    "        \"controlled\": args[\"controlled\"]\n",
    "    }\n",
    "    ## getting sample batch\n",
    "    batch = experiment.get_batch(train = False, idx = args[\"initial_time_step\"])\n",
    "    batch = move_batch_to_device(batch, experiment.device) \n",
    "    ## sampling from model\n",
    "    sample_dict = experiment.ar_sample(batch, config = ar_sample_config)\n",
    "    ## parsing samples dict\n",
    "    ar_samples = sample_dict[\"ar_samples\"]\n",
    "    ## displaying the shape of the results\n",
    "    print(f\"{ar_samples.shape=}\", end = \"\")\n",
    "    if args[\"full_out\"]:\n",
    "        trajectory = sample_dict[\"trajectory\"]\n",
    "        ## displaying the shape of the results\n",
    "        print(f\", {trajectory.shape=}\")\n",
    "    ## computing statistics over the mc samples\n",
    "    mean_ar_samples = torch.mean(ar_samples, dim = (0, 2))\n",
    "    std_ar_samples = torch.std(ar_samples, dim = (0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting example trajectory\n",
    "gt_trajectory = experiment.ssm.test_sim[\"latent_states\"][:ar_sample_config[\"num_ar_steps\"],:,:]\n",
    "observation_ids = np.random.randint(args[\"num_sims\"], size = (10))\n",
    "#observation_idx = 200\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "fig.suptitle(f\"AR vs GT States Simulation id: {observation_ids}\")\n",
    "for idx in range(10):\n",
    "    r_id = idx//5\n",
    "    c_id = idx%5\n",
    "    axes[r_id, c_id].plot(ar_samples[:, observation_ids[idx], ], label = \"Ar samples\", color = \"purple\")\n",
    "    axes[r_id, c_id].plot(gt_trajectory[:,observation_ids[idx], ], label = \"GT trajectory\", color = \"red\")\n",
    "    #axes.plot(ar_samples[:, :, 0], label = \"Ar samples\", color = \"purple\")\n",
    "    #axes.plot(gt_trajectory[:, :, 0], label = \"GT trajectory\", color = \"red\")\n",
    "    #legend = plt.legend()\n",
    "if args[\"log_results\"]:\n",
    "    experiment.writer.add_figure(\"ar_sampling/trajectories\", fig)\n",
    "else:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling over a grid of points\n",
    "CHECK_GRID = False\n",
    "if CHECK_GRID:\n",
    "    sample_config = {\n",
    "        \"num_time_steps\": args[\"num_time_steps\"],\n",
    "        \"num_samples\": args[\"num_samples\"]\n",
    "    }\n",
    "    ## defining the grid\n",
    "    lbound = -400\n",
    "    ubound = 400\n",
    "    num_points = 20\n",
    "    x_grid = torch.linspace(lbound, ubound, num_points)\n",
    "    x_grid = torch.unsqueeze(x_grid, dim = 1)\n",
    "    print(x_grid.shape)\n",
    "    batch = {\"x0\": x_grid, \"xc\": x_grid, \"y\": torch.zeros_like(x_grid)}\n",
    "    batch = move_batch_to_device(batch, experiment.device)\n",
    "    x1_hat = experiment.sample(batch, sample_config)\n",
    "    ## joyplot\n",
    "    observation_indices = np.arange(num_points)\n",
    "    samples = x1_hat[\"samples\"]\n",
    "    samples = torch.squeeze(samples)\n",
    "    df = pd.DataFrame(samples, columns = observation_indices)\n",
    "    fig, axes = joypy.joyplot(df, ylabels=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
